{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfidf_BOW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzAbmY1y7TEE"
      },
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5cdioyufIUv"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0SrZD3MmYcq"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cqayzTqAChu"
      },
      "source": [
        "## reading training data. This was used initally to test the accuracy using cross validation . Do not run this for the test data run\n",
        "data=open(\"/content/drive/MyDrive/Training Data.txt\",\"r\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKv7MoVrCsjp"
      },
      "source": [
        "## reading training data. This was used initally to test the accuracy using cross validation . Do not run this for the test data run\n",
        "data=data.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h4t7wM2DS5u"
      },
      "source": [
        "## d is the array form of the reviews in the training data\n",
        "d=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bm1RNlzDECa"
      },
      "source": [
        "## reading training data. This was used initally to test the accuracy using cross validation . Do not run this for the test data run\n",
        "## reading only the reviews into d without the class identifiers +1/-1\n",
        "for l in data:\n",
        "    s=str(l[2:len(l)-6])\n",
        "    d.append(s)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZkLB6cH3CeX"
      },
      "source": [
        "## reading test data. Run this for the test data run\n",
        "test_data=open(\"/content/drive/MyDrive/TestData.txt\",\"r\")\n",
        "test_data=test_data.readlines()\n",
        "## td is the array form of the reviews in the test data\n",
        "td=[]\n",
        "for l in test_data:\n",
        "    s=str(l[2:len(l)-6])\n",
        "    td.append(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e3PXzqRMENf"
      },
      "source": [
        "## y holds the class data of each review in the training data \n",
        "y=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY-iol2DL9Yi"
      },
      "source": [
        "for l in data:\n",
        "    s=int(l[0:2])\n",
        "    y.append(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApX1c2oNMOrd"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QJREYfpTc-E"
      },
      "source": [
        "dataset=list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEzNBDC4CFQs"
      },
      "source": [
        "dataset = td ## dataset was set to be equal to \"d\" when this program was initially run with training data to check the accuracy using cross validation\n",
        "## preprocessing the data to remove stop words, html tags and other punctuations\n",
        "for i in range(len(dataset)):\n",
        "    \n",
        "    dataset[i] = dataset[i].lower()\n",
        "    \n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
        "\n",
        "    dataset[i] = re.sub(pattern,\"\",dataset[i])\n",
        "    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
        "    dataset[i] = re.sub(r'<[a-b]+>', ' ', dataset[i])\n",
        "    #dataset[i] = re.sub(r'', ' ', dataset[i])\n",
        "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qTdxz93jKYK"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "## TF-IDF library used\n",
        "vectorizer = TfidfVectorizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho7cnoLIMKqU"
      },
      "source": [
        "## X has the TF-IDF for the training data. Dont run for test data run\n",
        "X = vectorizer.fit_transform(dataset)\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9Vex39wpOy1"
      },
      "source": [
        "## XT has the TF-IDF for the test data. Run this for test data run\n",
        "XT = vectorizer.fit_transform(dataset)\n",
        "print(XT.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgdQlNsHnM_i"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import random as sparse_random\n",
        "##Dimensionality reduction using svd library\n",
        "svd = TruncatedSVD(n_components=20, n_iter=7, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62qCAbqMMb2B"
      },
      "source": [
        "## X has the reduced dimensionality for training data . Do not run for test data run\n",
        "X=svd.fit_transform(X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdk3V6iiMgVw"
      },
      "source": [
        "## XT has the reduced dimensionlity for test data . Run for test data run.\n",
        "XT=svd.fit_transform(XT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4bctNfOIdSu"
      },
      "source": [
        "def weighted_knn(xTrain,y_train, xTest,k):\n",
        "    xTrain=np.linalg.norm(xTrain,axis=1)#getting  unit vector + euclidean distance equivalent to cosine sim\n",
        "\n",
        "    xTest=np.linalg.norm(xTest,axis=1)\n",
        "    knn = KNeighborsClassifier(n_neighbors=k, weights='distance', algorithm='brute', leaf_size=30, p=2, metric='euclidean', metric_params=None, )\n",
        "    \n",
        "    knn.fit(X_train, y_train)    \n",
        "    return knn.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK6SNjRPIgHM"
      },
      "source": [
        "#from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "#will first check which is the best k\n",
        "def pred(Ks,step):\n",
        "    \n",
        "    mean_acc = np.zeros((Ks-1))\n",
        "    std_acc = np.zeros((Ks-1))\n",
        "  \n",
        "  \n",
        "    for n in range(1,Ks,1):    \n",
        "        \n",
        "        yhat=weighted_knn(X_train,y_train,X_test,n)\n",
        "        #yhat=weighted(X_train,y_train,X_test,n)\n",
        "        mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)    \n",
        "        std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
        "\n",
        "    print( \"The best accuracy was:\", np.round(mean_acc.max()*100,2), \"% with k=\", mean_acc.argmax()+1) \n",
        "\n",
        "    plt.plot(range(1,Ks),mean_acc,'g')\n",
        "    plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.05)\n",
        "    plt.legend(('Accuracy ', '+/- 3xstd'))\n",
        "    plt.ylabel('Accuracy ')\n",
        "    plt.xlabel('Number of Neighbors (k)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMSfbnmSIl-q"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "## K fold cross validation done here . Do not run this for test data run\n",
        "y = np.array(y.copy())\n",
        "kf = KFold(n_splits=3,shuffle=True)\n",
        "kf.get_n_splits(X)\n",
        "Ks=30\n",
        "step=1\n",
        "print(kf)\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "        X_train, X_test = X_t[train_index], X_t[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        pred(Ks,step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO3AGB8hOhP3"
      },
      "source": [
        "xTest.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag41zzb4z30A"
      },
      "source": [
        "## Run this for test data run\n",
        "xTrain=X\n",
        "xTest=XT\n",
        "y_tain=y\n",
        "xTrain=np.linalg.norm(xTrain,axis=1)\n",
        "\n",
        "xTest=np.linalg.norm(xTest,axis=1)\n",
        "knn = KNeighborsClassifier(n_neighbors=24, weights='distance', algorithm='brute', leaf_size=30, p=2, metric='euclidean', metric_params=None, )\n",
        "\n",
        "knn.fit(xTrain, y_tain)    \n",
        "   \n",
        "result=knn.predict(xTest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huEk92G_r3nR"
      },
      "source": [
        "len(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQj2FU1NqE3j"
      },
      "source": [
        "## result file created here . Run this for test data run\n",
        "f = open(\"result_tv_weighted_cosine.txt\", \"w\")\n",
        "for i in list(result):\n",
        "    if i==1:\n",
        "        f.write(\"+1\\n\")\n",
        "    else:\n",
        "        f.write(\"-1\\n\")\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmBLuOolODW7"
      },
      "source": [
        "## do not run anything after this line\n",
        "### These functions were tried and experimented with during but did not work as expected and was not able to debug them\n",
        "### Finally used the knn library function to implement the algorithm\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "import sklearn\n",
        "COSINE SIMILARITY\n",
        "def knn(xTrain, xTest, k):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Finds the k nearest neighbors of xTest in xTrain.\n",
        "    Input:\n",
        "    xTrain = n x d matrix. n=rows and d=features\n",
        "    xTest = m x d matrix. m=rows and d=features (same amount of features as xTrain)\n",
        "    k = number of nearest neighbors to be found\n",
        "    Output:\n",
        "    dists = distances between xTrain/xTest points. Size of n x m\n",
        "    indices = kxm matrix with indices of yTrain labels\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    #distances=sklearn.metrics.pairwise.cosine_similarity(xTrain, Y=xTest, dense_output=True)\n",
        "    #print(\"cosine sim shape\",distance.shape())\n",
        "    dot = np.dot(xTrain, xTest.T)\n",
        "    norma = np.linalg.norm(xTrain)\n",
        "    normb = np.linalg.norm(xTest)\n",
        "    distances = dot / (norma * normb)\n",
        "    indices = np.argsort(distances, 0) #get indices of sorted items\n",
        "    distances = np.sort(distances,0) #distances sorted in axis 0\n",
        "    #returning the top-k closest distances.\n",
        "    return indices[0:k, : ], distances[0:k, : ]\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def knn(xTrain, xTest, k):\n",
        "    \"\"\"\n",
        "    \"\"\"Finds the k nearest neighbors of xTest in xTrain.\n",
        "    Input:\n",
        "    xTrain = n x d matrix. n=rows and d=features\n",
        "    xTest = m x d matrix. m=rows and d=features (same amount of features as xTrain)\n",
        "    k = number of nearest neighbors to be found\n",
        "    Output:\n",
        "    dists = distances between xTrain/xTest points. Size of n x m\n",
        "    indices = kxm matrix with indices of yTrain labels\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    #the following formula calculates the Euclidean distances.\n",
        "    distances = -2 * xTrain@xTest.T + np.sum(xTest**2,axis=1) + np.sum(xTrain**2,axis=1)[:, np.newaxis]\n",
        "    #because of numpy precision, some really small numbers might \n",
        "    #become negatives. So, the following is required.\n",
        "    distances[distances < 0] = 0\n",
        "    #for speed you can avoid the square root since it won't affect\n",
        "    #the result, but apply it for exact distances.\n",
        "    distances = distances**.5\n",
        "    indices = np.argsort(distances, 0) #get indices of sorted items\n",
        "    distances = np.sort(distances,0) #distances sorted in axis 0\n",
        "    #returning the top-k closest distances.\n",
        "    return indices[0:k, : ], distances[0:k, : ]\"\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "def weighted_knn_predictions(xTrain,yTrain,xTest,k=3,weight=1):\n",
        "    \"\"\"\n",
        "    \"\"\"Input:\n",
        "    xTrain = n x d matrix. n=rows and d=features\n",
        "    yTrain = n x 1 array. n=rows with label value\n",
        "    xTest = m x d matrix. m=rows and d=features\n",
        "    k = number of nearest neighbors to be found\n",
        "    Output:\n",
        "    predictions = predicted labels, ie preds(i) is the predicted label of xTest(i,:)\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    indices, distances = knn(xTrain,xTest,k)\n",
        "    yTrain = yTrain.flatten()\n",
        "    rows, columns = indices.shape\n",
        "    predictions = list()\n",
        "    w1=0\n",
        "    w2=0\n",
        "    for j in range(columns):\n",
        "        temp = list()\n",
        "        for i in range(rows):\n",
        "            cell = indices[i][j]\n",
        "            if yTrain[cell] == 1:\n",
        "                w1+=distances[i][j]\n",
        "            else:\n",
        "                w2+=distances[i][j]\n",
        "        if w1 >= w2:\n",
        "            predictions.append(1) \n",
        "        else:\n",
        "            predictions.append(-1)\n",
        "    predictions=np.array(predictions)\n",
        "    return predictions\"\"\"\n",
        "\"\"\"\n",
        "## count vectorizer was used before finalising with the tfid\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(dataset)\n",
        "XT=vectorizer.fit_transform(dataset)\n",
        "\"\"\"\"\"\n",
        "\n",
        "\"\"\"\"\n",
        "##The bag of words model was also used with a good level of accuracy\n",
        "# Creating the Bag of Words model\n",
        "word2count = {}\n",
        "for data in dataset:\n",
        "    words = nltk.word_tokenize(data)\n",
        "    for word in words:\n",
        "        if word not in word2count.keys():\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "            \"\"\"\n",
        "\"\"\"\n",
        "## 150 most used words were filtered from this before the TF-IDF approach\n",
        "import heapq\n",
        "freq_words = heapq.nlargest(150, word2count, key=word2count.get)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "## with word tokenisation\n",
        "X = []\n",
        "for data in dataset:\n",
        "    vector = []\n",
        "    for word in freq_words :\n",
        "        if word in nltk.word_tokenize(data):\n",
        "            vector.append(1)\n",
        "        else:\n",
        "            vector.append(0)\n",
        "    X.append(vector)\n",
        "X = np.asarray(X)\n",
        "\"\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}